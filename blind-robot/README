0506:Indiana: The Development of Embodied Cognition: Six Lessons from Babies.pdf
1509:Cornell: Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories.pdf
1601:Cornell: Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions.pdf
1606:ISI: Natural Language Communication with Robots.pdf
1805:KUISAI: A new dataset and model for learning to understand navigational instructions.pdf
1809:Hacettepe: RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes.pdf
1811:Facebook: 3D human pose estimation in video with temporal convolutions and semi-supervised training.pdf
1910:Cornell: Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight.pdf
1911:USC: IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks.pdf
2003:Tsinghua: MQA: Answering the Question via Robotic Manipulation.pdf
2005:Google: Language Conditioned Imitation Learning over Unstructured Data.pdf
2005:KTH: Chinese Whispers: A Multimodal Dataset for Embodied Language Grounding.pdf
2007:Australian: The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose.pdf
2009:DeepMind: Grounded Language Learning Fast and Slow.pdf
2010:Arizona: Language-Conditioned Imitation Learning for Robot Manipulation Tasks.pdf
2109:Stanford: Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation.pdf
2204:Google: Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.pdf
2205:Berkeley: Voxel-informed Language Grounding.pdf
2208:Singapore: MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model.pdf
2209:Berkeley: Supervised Learning of Behaviors DeepRLCourse Levine Lec-2.pdf. https://rail.eecs.berkeley.edu/deeprlcourse Lec 2 talks about imitation learning. Collect data from expert and use supervised training on obs,act pairs. Problem is distributional shift, imperfect learnt policy will end up with observations never seen before leading to even bigger errors. One solution is Dagger: instead of collecting data from expert, collect data from learnt policy and use expert to relabel. Analysis says if epsilon is the probability we make a mistake, then the expected total cost is O(εT²). If we can make Ptrain(state) = Ppolicy(state), e.g. with dagger, this can come down to O(εT). Finally, another idea is to just let the robot play and whatever state it reaches make that the target state for training (goal-conditioned behavioral cloning). "1903:Google: Learning Latent Plans from Play.pdf" is an example of this.
2210:Google: Interactive Language: Talking to Robots in Real Time.pdf.  Nice large training set for table top object pushing robot. Both real and simulated data. Language training data generated by after-the-fact annotation, not real time commands (i.e. motion comes first, language later in data generation, though annotators are instructed to use “command” language to describe what they see). Limitations: similar to our Bisk paper, only 2D arrangements. Only one type of action, i.e. push, no grasp or lift etc. Only low level instruction, e.g. push X to the Y of Z etc, no “make a line” or “separate by color”. These more abstract goals are tested by operators giving 5mins of low level commands.
2210:Stanford: VIMA: General Robot Manipulation with Multimodal Prompts.pdf

