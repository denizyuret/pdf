2011:UNC: Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision.pdf	The idea is to have a vocabulary of 50K pictures (vokens) and to associate each token in context with one of them. The token-voken association model is learnt with weak supervision from image-captioning datasets using MLPs to map BERT verbal representations with ResNext visual representations. This model is then used to determine gold vokens for a text-only dataset like wikipedia. BERT is trained on voken prediction as well as masked word prediction. This surpasses BERT only trained on masked words on text-only tasks like GLUE and Squad. Seems to implement imagination part of the imagination-perception loop: given a token, imagine a picture (picking from a fixed set). However perception is only used during the training of token-voken association model, not during downstream tasks. So once the associations are determined, the pictures may as well be discrete tokens like postags, they do not aid inference. The only difference during inference is token embeddings that have been biased to approximate picture embeddings (which for example may push synonyms together). It is not clear whether pictures are doing anything picture-like, could we just achieve similar results using wsd tags? Furthermore vokens are picked for every token, not just concrete words, this probably hurts performance by forcing irrelevant embeddings (e.g. function words and pics) together, easily fixed by applying this technique to only a subset of the tokens.
2106:UCSB: ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation.pdf
2112:UNC: VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks.pdf
2206:MIT: VALHALLA: Visual Hallucination for Machine Translation.pdf
2209:UNC: StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation.pdf
2210:UCSB: Visualize Before You Write: Imagination-Guided Open-Ended Text Generation.pdf
README
